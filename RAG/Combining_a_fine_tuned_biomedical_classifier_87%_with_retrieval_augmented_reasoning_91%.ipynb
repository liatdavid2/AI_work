{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZANp78_ccJkt"
   },
   "source": [
    "# The project: Biomedical Question Answering with Fine-tuned PubMedBERT and Agent-based RAG\n",
    "\n",
    "This project combines two key components to build a biomedical QA pipeline using the PubMedQA dataset:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Fine-tuning PubMedBERT\n",
    "\n",
    "- The PubMedQA dataset (`pqa_labeled`) is used, containing biomedical yes/no/maybe questions with evidence-based contexts and final decisions.\n",
    "- The model `microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext` is fine-tuned using Huggingface's `Trainer`.\n",
    "- The input pairs are: question + context → label (`yes`, `no`, or `maybe`).\n",
    "- Accuracy and a full classification report (precision, recall, F1) are calculated after 3 epochs of training.\n",
    "- Fine-tuned weights are saved locally for downstream use.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. LangGraph Agent-based RAG Pipeline\n",
    "\n",
    "- A LangGraph pipeline is constructed with 5 agents:\n",
    "  - **RetrieverAgent**: Uses SentenceTransformer to retrieve top 3 similar contexts from the dataset using FAISS.\n",
    "  - **ContextPrintAgent**: (silent) for structure, optionally prints top contexts.\n",
    "  - **ClassifierAgent**: Loads the fine-tuned PubMedBERT model to predict the answer based on the concatenated context.\n",
    "  - **ExplainerAgent**: Uses an LLM (e.g. Granite) to explain why the model chose that answer based on the retrieved context.\n",
    "  - **FinalOutputAgent**: Prints the full reasoning, context chunks, model prediction, and ground truth (if available).\n",
    "\n",
    "- Text is formatted to break every 10 words for readability.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Output\n",
    "\n",
    "For a question like:\n",
    "> \"Can vitamin D deficiency cause depression?\"\n",
    "\n",
    "The pipeline retrieves relevant studies, predicts `\"yes\"`, and explains the answer based on associated biomedical contexts. If the question exists in the dataset, the true label is also shown.\n",
    "\n",
    "---\n",
    "\n",
    "### Goal\n",
    "\n",
    "This project demonstrates how combining a **fine-tuned biomedical classifier** with **retrieval-augmented reasoning** and **explainable LLM output** can improve trust and performance in biomedical QA systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pevRT914f6AL"
   },
   "source": [
    "## The dataset: pubmed_qa\n",
    "\n",
    "This dataset contains biomedical question-answer pairs derived from PubMed articles, used for training models in biomedical **yes/no/maybe** question answering.\n",
    "\n",
    "### What the dataset contains\n",
    "\n",
    "Each row in the `\"train\"` split includes the following fields:\n",
    "\n",
    "| Column Name      | Description                                                  |\n",
    "| ---------------- | ------------------------------------------------------------ |\n",
    "| `pubid`          | Unique PubMed article identifier                             |\n",
    "| `question`       | A biomedical yes/no/maybe question derived from the article  |\n",
    "| `context`        | A dictionary with supporting article text and metadata       |\n",
    "| `long_answer`    | A full explanatory answer in free text form                  |\n",
    "| `final_decision` | The ground-truth label: one of `\"yes\"`, `\"no\"`, or `\"maybe\"` |\n",
    "\n",
    "### Details of the `context` field\n",
    "\n",
    "The `context` field is a dictionary that includes:\n",
    "\n",
    "* **`contexts`**: List of supporting paragraph texts (strings)\n",
    "* **`labels`**: Section labels such as `\"METHODS\"`, `\"RESULTS\"`, etc.\n",
    "* **`meshes`**: MeSH terms (medical subject headings) for the article\n",
    "* **`reasoning_required_pred`**: Predicted answer based on full reasoning\n",
    "* **`reasoning_free_pred`**: Prediction assuming minimal reasoning\n",
    "\n",
    "### Label for classification\n",
    "\n",
    "The main label for supervised training is:\n",
    "\n",
    "* **`final_decision`**: The correct answer to the question (used as the classification target)\n",
    "\n",
    "Typical mapping for training:\n",
    "\n",
    "```python\n",
    "label2id = {\"no\": 0, \"yes\": 1, \"maybe\": 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSyJvhDQIVN8"
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes accelerate transformers langgraph faiss-cpu evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nDdKciZTIVMI"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\"env\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "1dd-GKKBP88z",
    "outputId": "6991f8d0-8b59-48f1-f3ac-3d693c28e6ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-4282328212.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='565' max='565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [565/565 01:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.521000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classification Report (fine-tuned BERT on PubMedQA):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.89      0.85      0.87        40\n",
      "         yes       0.84      0.96      0.90        45\n",
      "       maybe       0.91      0.67      0.77        15\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.88      0.82      0.85       100\n",
      "weighted avg       0.87      0.87      0.87       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ PubMedQA Fine-tuning with Huggingface Trainer\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: Load PubMedQA\n",
    "# ------------------------------\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "dataset = dataset.filter(lambda x: x[\"final_decision\"] in [\"yes\", \"no\", \"maybe\"])\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: Encode labels\n",
    "# ------------------------------\n",
    "label2id = {\"no\": 0, \"yes\": 1, \"maybe\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "def encode_labels(example):\n",
    "    example[\"label\"] = label2id[example[\"final_decision\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(encode_labels)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: Tokenization\n",
    "# ------------------------------\n",
    "model_ckpt = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tokenizer(\n",
    "        list(map(str, batch[\"question\"])),\n",
    "        list(map(str, batch[\"context\"])),\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenized = dataset.map(preprocess, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 4: Load model\n",
    "# ------------------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 5: Metrics\n",
    "# ------------------------------\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    }\n",
    "\n",
    "# ------------------------------\n",
    "# Step 6: Training setup\n",
    "# ------------------------------\n",
    "train_test = tokenized[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./pubmedqa-bert\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_test[\"train\"],\n",
    "    eval_dataset=train_test[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 7: Train\n",
    "# ------------------------------\n",
    "trainer.train()\n",
    "\n",
    "# ------------------------------\n",
    "# Step 8: Evaluate + F1\n",
    "# ------------------------------\n",
    "predictions = trainer.predict(train_test[\"test\"])\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "true = predictions.label_ids\n",
    "\n",
    "print(\"✅ Classification Report (fine-tuned BERT on PubMedQA):\")\n",
    "print(classification_report(true, preds, target_names=[\"no\", \"yes\", \"maybe\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXyHedsXRhVE",
    "outputId": "66a651b6-f5a4-46ac-b81b-d015821cd2c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./pubmedqa-bert/tokenizer_config.json',\n",
       " './pubmedqa-bert/special_tokens_map.json',\n",
       " './pubmedqa-bert/vocab.txt',\n",
       " './pubmedqa-bert/added_tokens.json',\n",
       " './pubmedqa-bert/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer after training\n",
    "model.save_pretrained(\"./pubmedqa-bert\")\n",
    "tokenizer.save_pretrained(\"./pubmedqa-bert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdjNavG7loGL"
   },
   "source": [
    "## The Evaluation in This Script: RAG-style Classification\n",
    "\n",
    "This script performs evaluation of a fine-tuned BERT model using a **Retrieval-Augmented Generation (RAG)** style workflow, rather than the standard evaluation performed via `Trainer.evaluate()`.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Retriever with FAISS + SentenceTransformer**:\n",
    "   - Each question is encoded with a SentenceTransformer.\n",
    "   - The script uses a FAISS index to retrieve the top-3 most semantically similar contexts from the dataset.\n",
    "   - These contexts are concatenated to form a single input passage.\n",
    "\n",
    "2. **Classification using Fine-tuned BERT**:\n",
    "   - The concatenated context (from retrieval) and the question are passed into a fine-tuned `BERT` model (`pubmedqa-bert`) for classification.\n",
    "   - The model predicts one of three labels: `yes`, `no`, or `maybe`.\n",
    "\n",
    "3. **Evaluation Metrics**:\n",
    "   - Predictions are compared to the original ground-truth labels.\n",
    "   - Standard classification metrics are reported:\n",
    "     - Precision\n",
    "     - Recall\n",
    "     - F1-score\n",
    "     - Accuracy (via `sklearn.metrics.classification_report`)\n",
    "\n",
    "### Difference from Standard Evaluation\n",
    "\n",
    "| Feature                | Trainer.evaluate()                             | RAG Evaluation (this script)                         |\n",
    "|------------------------|------------------------------------------------|------------------------------------------------------|\n",
    "| Input Context          | Uses original `context` from the dataset       | Uses top-3 retrieved contexts from FAISS             |\n",
    "| Retrieval involved     | No                                             | Yes                                                  |\n",
    "| Realistic QA scenario  | Limited                                        | More realistic (simulates open-domain QA setup)      |\n",
    "| Evaluation target      | Pure model performance on static input         | End-to-end performance under retrieval + reasoning   |\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This type of evaluation answers the question:\n",
    "\n",
    "> *\"How well does the fine-tuned model perform when it has to rely on dynamically retrieved context instead of ground-truth context?\"*\n",
    "\n",
    "This is particularly useful for applications involving **retrieval-based question answering**, such as biomedical QA systems built over large corpora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257,
     "referenced_widgets": [
      "92df393a29684160a10b4f95b9ef9ba8",
      "e911059f24f742b29031a55435d2e0bd",
      "1ee6d7e7a45c4f26aed20aca940917fa",
      "8688d892965c4bd99552e98a28ad84c4",
      "1c2c66bb67684ae7a841afa1c670c69b",
      "04944e7164104d75b991b1c715491e82",
      "3489738666da44eb9637664f637e9629",
      "cbd44cef0998401ca4d057fb28deb116",
      "3f0ac51b1efe434f8ed91f705a3c0995",
      "eda44f07d1774f5982af62be9f0f4c6f",
      "731bf1ae453d4b9899138d7bd1335d9b"
     ]
    },
    "id": "sUrRgiA7SG_c",
    "outputId": "cd3bda76-f7a0-4f00-f362-d2be784530c4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92df393a29684160a10b4f95b9ef9ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating RAG: 100%|██████████| 1000/1000 [03:48<00:00,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG Evaluation – PubMedQA (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.96      0.92      0.94       338\n",
      "         yes       0.92      0.94      0.93       552\n",
      "       maybe       0.74      0.74      0.74       110\n",
      "\n",
      "    accuracy                           0.91      1000\n",
      "   macro avg       0.87      0.87      0.87      1000\n",
      "weighted avg       0.91      0.91      0.91      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------\n",
    "# Load model + tokenizer\n",
    "# ------------------------------\n",
    "model_path = \"./pubmedqa-bert\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "label_map = {0: \"no\", 1: \"yes\", 2: \"maybe\"}\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# ------------------------------\n",
    "# Load dataset\n",
    "# ------------------------------\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\", split=\"train\")\n",
    "dataset = dataset.filter(lambda x: x[\"final_decision\"] in [\"yes\", \"no\", \"maybe\"])\n",
    "dataset = dataset.select(range(1000))  # ניתן לשנות\n",
    "contexts = [str(x) for x in dataset[\"context\"]]\n",
    "questions = dataset[\"question\"]\n",
    "true_labels = [x.lower() for x in dataset[\"final_decision\"]]\n",
    "\n",
    "# ------------------------------\n",
    "# FAISS Retriever\n",
    "# ------------------------------\n",
    "retriever = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "context_embeddings = retriever.encode(contexts, convert_to_numpy=True, show_progress_bar=True)\n",
    "index = faiss.IndexFlatIP(context_embeddings.shape[1])\n",
    "index.add(context_embeddings)\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluate on full dataset\n",
    "# ------------------------------\n",
    "preds = []\n",
    "\n",
    "for q in tqdm(questions, desc=\"Evaluating RAG\"):\n",
    "    # Retrieve top-3 contexts\n",
    "    q_emb = retriever.encode([q], convert_to_numpy=True)\n",
    "    D, I = index.search(q_emb, 3)\n",
    "    top_contexts = [str(contexts[int(i)]) for i in I[0]]\n",
    "    full_context = \" \".join(top_contexts)\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        q,\n",
    "        full_context,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "        preds.append(label_map[pred])\n",
    "\n",
    "\n",
    "inv_label_map = {\"no\": 0, \"yes\": 1, \"maybe\": 2}\n",
    "\n",
    "true_numeric = [inv_label_map[x] for x in true_labels]\n",
    "preds = [inv_label_map[p] for p in preds]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"✅ RAG Evaluation – PubMedQA (Test Set):\")\n",
    "print(classification_report(true_numeric, preds, target_names=[\"no\", \"yes\", \"maybe\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebW6HIOBUDz0"
   },
   "source": [
    "#AI AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290,
     "referenced_widgets": [
      "3ec0c5e5f5284761a391d50fddfc667a",
      "b5ce225aba644398b5bed7a8c547c53a",
      "cb5ab1e0416446a4a361f219ef9c9530",
      "2c91cce6c21a4cf9b1e819c54f3ddbc3",
      "395beb4506104c919b30e11d70ed2785",
      "1410e95bcfdc4cf4b3a04cad11a348fa",
      "002b571f56254a3ea35469a61c50329c",
      "33d18791ecc64fd2a9a185d7f9a23b46",
      "b7b0ac7e25eb4c2b95ce80654ce1ac8b",
      "14c5cc3e534644a6a31bf92623217d7e",
      "ced2845e5eef41aeb3081f9eb8f0a71d",
      "63510a21d3064aeabb0ad0c0c97a8948",
      "f2d22aecdc114eff9fbbebccab926fff",
      "7ff86e399f8547f5889b8c2982fae3ce",
      "028a991bdf6a4569b6e8b549b46c56dd",
      "c3bbf4812f924ae1a4e3c3955a6b16e7",
      "e6bc1e2d65074182a7de0d22f16e9c13",
      "4d62c203af2f42d0aa44d67076a1774a",
      "ac3e9cfbaec148d9b3c3ed797ef0d43a",
      "b7715ee0a45146998b53bef31b941595",
      "3a00c5d28e264c0bbee2bf9479dfa7a2",
      "4137d73dfe0e44a39c009296c0692ad8",
      "db0eb429fc194b00932c2e39a31e7ea6",
      "661066f8663a49e19accc3c8ceea16de",
      "087e6ba46af24b328f80d72842432204",
      "2413710e73b7403eaae6135209dcd07f",
      "217d7b22733b4edc995f4f36e2c13d20",
      "c080f1e4b55644bbaa32716f20f5f70b",
      "f2738163a2994e54b277b4526eab34e2",
      "a5c2d0dffae540f198fc707f2f3ecaa4",
      "591df710a32f4ddfbb1a64cb2e482cb6",
      "7c4bb75b9c504a2bb3baa7819da3482b",
      "da40caf534df4d7dba8eb91cad64c483",
      "afd53828acf44953abc1ce8cbc4eebd6",
      "f510bff3cb1a4a77baa55fc71d1958b9",
      "4c8e6a9375b1432185b2312233a40682",
      "996541aa0f5d443180ce505a60968c2e",
      "52f3431a3c2e4e3ebe4917d0b83c3f04",
      "cde45f5973b949af819000850fd4704a",
      "c18be49a0c57434da3fca808a1a83312",
      "ea449de3d00144878c49e41958940f1b",
      "20336067d6474684a331bd151361290e",
      "82048d6b8bff414991bc54f83b3dbe40",
      "e7aafa1c81f14b0992fece8bc0e7fac6",
      "5487788db82547b78fc8e5d0998e7140",
      "d564214f132d4704a4a97053bfd760a9",
      "86b6d42e9d524a16ba2ad20afd8d8236",
      "fba3a3f1f68041e4b7ebf58463e54e05",
      "c0e70adbaf00468692822fafb1b42762",
      "3800526558b94b97a238fb4ed04094e3",
      "93f957c8c9fc49ca87fb11f8e97db5d7",
      "f966ef4ad93d4b7b8490a117a853e22d",
      "37c83eb05d4945fdb24527e57fe7a928",
      "e8d6cfe57115458b88dcc7198efa12f3",
      "83097431ac8b41ed8e768d66521bc9bd",
      "7f3419d764cf4b268d186149617f4cbc",
      "cecb2d1ffd364d72956050d47b01ac01",
      "50f822d008c94c41baae765aaa441c4f",
      "f006375d6f144c1d93121386507784b9",
      "431e93059c2c4f0bba241bf710c6aa1c",
      "b3f4ce08ad134f6998ebdcbd6b8168ea",
      "00322240f7164cc08372ed412d50a7f9",
      "8e410e4ac17148d9b88eadd318bc74ff",
      "d0eb8ad0c9404c53a00e16a47010ab1a",
      "863430798e324ff98bc078d60b5f844c",
      "c321c4d7a93d49b78e476b5652ccf2c6",
      "86464f10ab1d4f39ae12819b2dbd084c",
      "846055abd1944be0bb02ac94fb9abcec",
      "6e26922b1aad45d0808ac8b127076bad",
      "5433efbb573c453ea4a62b0e855144ae",
      "162b1d9abc744c6ba89e09aa51155b61",
      "baa81a9436354947b3c1580921c88d47",
      "46b0223e81d7477b857afdc1d4c4b850",
      "5941c1cac439406c920127dfc77fb4f4",
      "2f851b16b9df4de388e8852ca7270d23",
      "74f2524b2a704969975b3a7834ecd2e0",
      "e3da4471939245e7bf24f8bbdfbc87cd",
      "ae6857c0b2f643f2acc1646c72c131d1",
      "5561ecc0a5d1479c84543f0c4b32ba50",
      "fd717ecd92074c6f940369d7f3b28d54",
      "94861c74552e4395844f0643f4797707",
      "24288b4e9e02402d970bb26cef32d9d0",
      "89f81acde3f14f29a29d06e3762a6d6d",
      "6bb1058287b44d0da98f6d5bf99fe202",
      "c0962473bf9f4fd99392fc406f2e3f41",
      "db2d2a7767864d3b85b4259c14a5a81a",
      "4bd19586f91e4e44b07113cbd5947204",
      "326200e44885481aaf13499a612f5889"
     ]
    },
    "id": "hYYIrbTKJAVk",
    "outputId": "13844bc6-1662-4057-8f03-61885d5043d0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec0c5e5f5284761a391d50fddfc667a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63510a21d3064aeabb0ad0c0c97a8948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0eb429fc194b00932c2e39a31e7ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd53828acf44953abc1ce8cbc4eebd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5487788db82547b78fc8e5d0998e7140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3419d764cf4b268d186149617f4cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86464f10ab1d4f39ae12819b2dbd084c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/207 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6857c0b2f643f2acc1646c72c131d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    GRANITE_MODEL = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "\n",
    "    granite_model = AutoModelForCausalLM.from_pretrained(\n",
    "        GRANITE_MODEL,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    granite_tokenizer = AutoTokenizer.from_pretrained(GRANITE_MODEL)\n",
    "\n",
    "    granite_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=granite_model,\n",
    "        tokenizer=granite_tokenizer,\n",
    "        pad_token_id=granite_tokenizer.eos_token_id,\n",
    "        return_full_text=False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"[Warning] Failed to load Granite model:\", e)\n",
    "    granite_pipe = lambda prompt, **kwargs: [{\"generated_text\": \"[granite model unavailable]\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 813,
     "referenced_widgets": [
      "c4d18248b6524cabb9ba52bf91645bf2",
      "77fa4e6a16dd4e89ac97637b9a9ae5a4",
      "ad7ee9d98ec349ac8518e1faa3421da4",
      "e003c392606943bd8e6895e8ca8e3d1c",
      "daeb13c0d54b4e7a8aa4ef4616ea7eda",
      "d19911d120aa4b24ba644c1d4ec75727",
      "39b024b6627840ebb1d7899dd8db0def",
      "6c548078ea904b85bdfda733cde29fa2",
      "ce1d9701bfc34e74a0b31f08327adb62",
      "750ca8d753624a1aa1b554be18cf77e5",
      "8f6f7e1c151245dc8e1f2bfafc54fab6"
     ]
    },
    "id": "XwY0rK1OXCAU",
    "outputId": "460f361a-9ded-4504-bb72-27d491f3f1d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Encoding context with SentenceTransformer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d18248b6524cabb9ba52bf91645bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading fine-tuned BERT classifier...\n",
      "\n",
      "==============================\n",
      "Question:\n",
      "Can vitamin D deficiency cause depression?\n",
      "\n",
      "Predicted Answer: yes\n",
      "Ground Truth: [unknown]\n",
      "\n",
      "Top Contexts:\n",
      "\n",
      "Context 1:\n",
      "The aetiology of osteochondritis dissecans is still unclear. The aim\n",
      "of this prospective pilot study was to analyse whether vitamin\n",
      "D insufficiency, or deficiency, might be a contributing etiological factor\n",
      "in the development of an OCD lesion.\n",
      "\n",
      "Context 2:\n",
      "The serum level of vitamin D3 in 23 consecutive patients\n",
      "(12 male and 11 female) suffering from a stage III,\n",
      "or stages III and IV, OCD lesion (mostly stage III)\n",
      "admitted for surgery was measured.\n",
      "\n",
      "Context 3:\n",
      "The patients' mean age was 31.3 years and most of\n",
      "them already exhibited closed epiphyseal plates. In the majority of\n",
      "patients (18/23), a distinct vitamin D3 deficiency was found, two\n",
      "patients were vitamin D3-insufficient and, in three patients, the vitamin\n",
      "D3 level reached the lowest normal value.\n",
      "\n",
      "Explanation:\n",
      "The context suggests that vitamin D deficiency is common in\n",
      "certain patient groups, including those with osteochondritis dissecans (Context 3)\n",
      "and chronic kidney disease (Context 8, Context 9). Additionally, context\n",
      "4 discusses the role of vitamin D in preventing autoimmunity,\n",
      "which includes autoimmune diseases like depression. Although the provided context\n",
      "does not directly demonstrate a cause-and-effect relationship between vitamin D\n",
      "deficiency and depression, it does support the possibility of an\n",
      "association, given that vitamin D deficiency has been linked to\n",
      "autoimmune conditions. Therefore, the classifier's answer of \"yes\" is appropriate\n",
      "based on the indirect evidence\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q langgraph sentence-transformers transformers datasets faiss-cpu\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, BitsAndBytesConfig\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# State definition\n",
    "# ------------------------------\n",
    "class QAState(TypedDict):\n",
    "    question: str\n",
    "    prediction: str\n",
    "    context: str\n",
    "    top_contexts: list[str]\n",
    "    ground_truth: str\n",
    "    explanation: str\n",
    "\n",
    "# ------------------------------\n",
    "# Load dataset + labels\n",
    "# ------------------------------\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\", split=\"train\")\n",
    "dataset = dataset.filter(lambda x: x[\"final_decision\"] in [\"yes\", \"no\", \"maybe\"])\n",
    "dataset = dataset.select(range(1000))\n",
    "\n",
    "questions_list = dataset[\"question\"]\n",
    "contexts_raw = dataset[\"context\"]\n",
    "labels_raw = dataset[\"final_decision\"]\n",
    "label_map = {0: \"no\", 1: \"yes\", 2: \"maybe\"}\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# ------------------------------\n",
    "# Encode context embeddings\n",
    "# ------------------------------\n",
    "print(\"Encoding context with SentenceTransformer...\")\n",
    "texts_for_index = [\n",
    "    \" \".join(x[\"contexts\"]) if isinstance(x, dict) and \"contexts\" in x else str(x)\n",
    "    for x in contexts_raw\n",
    "]\n",
    "retriever = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings = retriever.encode(texts_for_index, convert_to_numpy=True, show_progress_bar=True)\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "# ------------------------------\n",
    "# Load fine-tuned classifier\n",
    "# ------------------------------\n",
    "print(\"Loading fine-tuned BERT classifier...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./pubmedqa-bert\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./pubmedqa-bert\")\n",
    "\n",
    "def break_lines_every_n_words(text: str, n: int = 10) -> str:\n",
    "    \"\"\"Break text every n words.\"\"\"\n",
    "    words = text.split()\n",
    "    lines = [\" \".join(words[i:i+n]) for i in range(0, len(words), n)]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ------------------------------\n",
    "# Agent 1: Retriever\n",
    "# ------------------------------\n",
    "def RetrieverAgent(state: QAState) -> QAState:\n",
    "    q = state[\"question\"]\n",
    "    q_emb = retriever.encode([q], convert_to_numpy=True)\n",
    "    D, I = index.search(q_emb, 3)\n",
    "\n",
    "    top_contexts = []\n",
    "    for i in I[0]:\n",
    "        raw = contexts_raw[int(i)]\n",
    "        if isinstance(raw, dict) and \"contexts\" in raw:\n",
    "            top_contexts.extend(raw[\"contexts\"])\n",
    "        else:\n",
    "            top_contexts.append(str(raw))\n",
    "\n",
    "    full_context = \"\\n\\n\".join([f\"Context {i+1}:\\n{ctx}\" for i, ctx in enumerate(top_contexts)])\n",
    "    if q in questions_list:\n",
    "        gt = labels_raw[questions_list.index(q)]\n",
    "    else:\n",
    "        gt = \"[unknown]\"  # שאלה חדשה שלא קיימת בדאטהסט\n",
    "\n",
    "    return {**state, \"context\": full_context, \"top_contexts\": top_contexts, \"ground_truth\": gt}\n",
    "\n",
    "# ------------------------------\n",
    "# Agent 2: Context printer\n",
    "# ------------------------------\n",
    "def ContextPrintAgent(state: QAState) -> QAState:\n",
    "    for i, ctx in enumerate(state[\"top_contexts\"][:3]):\n",
    "      return state\n",
    "\n",
    "# ------------------------------\n",
    "# Agent 3: Classifier\n",
    "# ------------------------------\n",
    "def ClassifierAgent(state: QAState) -> QAState:\n",
    "    inputs = tokenizer(\n",
    "        state[\"question\"],\n",
    "        state[\"context\"],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "        label = label_map[pred]\n",
    "    return {**state, \"prediction\": label}\n",
    "\n",
    "# ------------------------------\n",
    "# Agent 4: LLM explainer\n",
    "# ------------------------------\n",
    "def ExplainerAgent(state: QAState) -> QAState:\n",
    "    prompt = f\"\"\"\n",
    "You are a biomedical QA assistant.\n",
    "\n",
    "Given the following question and retrieved context, the classifier predicted the answer: \"{state['prediction']}\".\n",
    "\n",
    "Your task is to explain WHY this answer is appropriate, based only on the evidence from the context.\n",
    "\n",
    "---\n",
    "\n",
    "Question:\n",
    "{state['question']}\n",
    "\n",
    "Context:\n",
    "{state['context']}\n",
    "\n",
    "Answer:\n",
    "{state['prediction']}\n",
    "\n",
    "---\n",
    "\n",
    "Explanation:\n",
    "\"\"\"\n",
    "    response = granite_pipe(prompt, max_new_tokens=150)[0][\"generated_text\"]\n",
    "    return {**state, \"explanation\": response.strip()}\n",
    "\n",
    "# ------------------------------\n",
    "# Agent 5: Ground truth compare + print final output\n",
    "# ------------------------------\n",
    "def FinalOutputAgent(state: QAState) -> QAState:\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Question:\\n{state['question']}\\n\")\n",
    "    print(f\"Predicted Answer: {state['prediction']}\")\n",
    "    print(f\"Ground Truth: {state['ground_truth']}\\n\")\n",
    "\n",
    "    print(f\"Top Contexts:\")\n",
    "    for i, ctx in enumerate(state['top_contexts'][:3]):\n",
    "        print(f\"\\nContext {i+1}:\\n{break_lines_every_n_words(ctx.strip(), 10)}\")\n",
    "\n",
    "    print(f\"\\nExplanation:\\n{break_lines_every_n_words(state['explanation'].strip(), 10)}\")\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Build LangGraph\n",
    "# ------------------------------\n",
    "graph = StateGraph(QAState)\n",
    "graph.add_node(\"RetrieverAgent\", RetrieverAgent)\n",
    "graph.add_node(\"ContextPrintAgent\", ContextPrintAgent)\n",
    "graph.add_node(\"ClassifierAgent\", ClassifierAgent)\n",
    "graph.add_node(\"ExplainerAgent\", ExplainerAgent)\n",
    "graph.add_node(\"FinalOutputAgent\", FinalOutputAgent)\n",
    "\n",
    "graph.set_entry_point(\"RetrieverAgent\")\n",
    "graph.add_edge(\"RetrieverAgent\", \"ContextPrintAgent\")\n",
    "graph.add_edge(\"ContextPrintAgent\", \"ClassifierAgent\")\n",
    "graph.add_edge(\"ClassifierAgent\", \"ExplainerAgent\")\n",
    "graph.add_edge(\"ExplainerAgent\", \"FinalOutputAgent\")\n",
    "graph.add_edge(\"FinalOutputAgent\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "# ------------------------------\n",
    "# Run full pipeline\n",
    "# ------------------------------\n",
    "question = \"Can vitamin D deficiency cause depression?\"\n",
    "_ = app.invoke({\"question\": question})  # discard output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWibj7O6ao_y",
    "outputId": "cb799ebe-ee57-4a81-84ed-5b95dec950ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: Can dobutamine stress echocardiography induce cardiac troponin elevation?\n",
      "\n",
      "==============================\n",
      "Question:\n",
      "Can dobutamine stress echocardiography induce cardiac troponin elevation?\n",
      "\n",
      "Predicted Answer: yes\n",
      "Ground Truth: no\n",
      "\n",
      "Top Contexts:\n",
      "\n",
      "Context 1:\n",
      "Elevation of cardiac troponin (cTn) is considered specific for myocardial\n",
      "damage. Elevated cTn and echocardiogrpahic documentation of wall motion abnormalities\n",
      "(WMAs) that were recorded after extreme physical effort raise the\n",
      "question whether dobutamine stress echo (DSE), can also induce elevation\n",
      "of troponin.\n",
      "\n",
      "Context 2:\n",
      "we prospective enrolled stable patients (age>18 years) referred to DSE.\n",
      "The exam was performed under standardized conditions. Blood samples for\n",
      "cTnI were obtained at baseline and 18-24 hours after the\n",
      "test. We aimed to compare between the clinical and echocardiographic\n",
      "features of patients with elevated cTnI and those without cTnI\n",
      "elevations.\n",
      "\n",
      "Context 3:\n",
      "Fifty-seven consecutive patients were included. The average age was 64.4\n",
      "± 10.7, 73% of the patients were males, and nearly\n",
      "half of the patients were known to have ischemic heart\n",
      "disease. Two of the patients were excluded due to technical\n",
      "difficulty. No signs of ischemia were recorded in 25 (45.4%).\n",
      "Among the patients with established ischemia on DSE, 12 (22%)\n",
      "had mild ischemia, 13 (23.6%) had moderate and 5 (9%)\n",
      "had severe ischemia. Angiography was performed in 13 (26%) of\n",
      "the patients, of which 7 had PCI and one was\n",
      "referred to bypass surgery. None of the patients had elevated\n",
      "cTnI 18-24 hours after the DSE.\n",
      "\n",
      "Explanation:\n",
      "The answer is appropriate because Context 1 directly addresses the\n",
      "question by stating that elevated cardiac troponin (cTn) is considered\n",
      "specific for myocardial damage and that dobutamine stress echo (DSE)\n",
      "can induce troponin elevation, as evidenced by the documentation of\n",
      "wall motion abnormalities after extreme physical effort. Context 2 sets\n",
      "the stage for a study on DSE, while Context 3\n",
      "further supports the idea that DSE can cause troponin elevation,\n",
      "as no patients in the study had elevated cTnI 18-24\n",
      "hours after the DSE. The remaining contexts do not directly\n",
      "address the question but provide background information on tropon\n",
      "==============================\n",
      "\n",
      "\n",
      "Question 2: Is a mandatory general surgery rotation necessary in the surgical clerkship?\n",
      "\n",
      "==============================\n",
      "Question:\n",
      "Is a mandatory general surgery rotation necessary in the surgical clerkship?\n",
      "\n",
      "Predicted Answer: no\n",
      "Ground Truth: no\n",
      "\n",
      "Top Contexts:\n",
      "\n",
      "Context 1:\n",
      "Changes in the spectrum of general surgery and the delivery\n",
      "of surgical care have placed the requirement for a mandatory\n",
      "general surgery rotation in the surgical clerkship in question.\n",
      "\n",
      "Context 2:\n",
      "We tested the hypothesis that equal mastery of surgical clerkship\n",
      "objectives can be obtained in a clerkship with and without\n",
      "general surgery. Students chose any two surgical rotations and were\n",
      "assessed by written examination, objective structured clinical examination (OSCE), ward\n",
      "evaluations, self-assessment objectives questionnaire, and satisfaction survey.\n",
      "\n",
      "Context 3:\n",
      "Data for 54 students showed no differences in scores between\n",
      "groups on any parameter. No specific concerns related to the\n",
      "absence of general surgery were identified.\n",
      "\n",
      "Explanation:\n",
      "The answer \"no\" is appropriate because Context 3 explicitly states\n",
      "that \"data for 54 students showed no differences in scores\n",
      "between groups on any parameter,\" referring to parameters such as\n",
      "written examination, OSCE, ward evaluations, self-assessment objectives questionnaire, and satisfaction\n",
      "survey. This indicates that a mandatory general surgery rotation in\n",
      "the surgical clerkship does not lead to better mastery of\n",
      "clerkship objectives. Context 1 also questions the necessity of a\n",
      "mandatory general surgery rotation, given changes in the field. Context\n",
      "9 supports this by showing that surgical education remained unchanged\n",
      "after the implementation of the 80-hour workweek, with only\n",
      "==============================\n",
      "\n",
      "\n",
      "Question 3: Necrotizing fasciitis: an indication for hyperbaric oxygenation therapy?\n",
      "\n",
      "==============================\n",
      "Question:\n",
      "Necrotizing fasciitis: an indication for hyperbaric oxygenation therapy?\n",
      "\n",
      "Predicted Answer: no\n",
      "Ground Truth: no\n",
      "\n",
      "Top Contexts:\n",
      "\n",
      "Context 1:\n",
      "The accepted treatment protocol for necrotizing fasciitis (NF) consists of\n",
      "extensive surgery and wide spectrum antibiotics. Hyperbaric oxygenation (HBO) has\n",
      "been recommended as adjuvant therapy for NF, improving patient mortality\n",
      "and outcome. However, the beneficial effect of HBO for NF\n",
      "remains controversial.\n",
      "\n",
      "Context 2:\n",
      "A retrospective evaluation of treatment outcome in 37 patients treated\n",
      "for NF between 1984 and 1993 was carried out. The\n",
      "mortality rate, morbidity criteria, and risk factors for grave prognosis\n",
      "were compared between a group of 25 patients who received\n",
      "HBO as part of their treatment protocol and a group\n",
      "of the remaining 12 patients treated by surgical excision and\n",
      "antibiotics alone.\n",
      "\n",
      "Context 3:\n",
      "The two groups were found to be similar with regard\n",
      "to age, gender, the incidence of individual risk factors for\n",
      "ominous prognosis, and the Acute Physiology and Chronic Health Evaluation\n",
      "(APACHE) II score for disease's severity on presentation. The mortality\n",
      "rate among the HBO-treated patients was 36%, as opposed to\n",
      "25% in the non-HBO group. The mean number of surgical\n",
      "débridements required per patient was significantly higher in the HBO\n",
      "group: 3.3 compared with 1.5 in the non-HBO-treated patients. Although\n",
      "the average length of hospitalization for survivors was shorter for\n",
      "the HBO group, the difference between the groups did not\n",
      "reach statistical significance.\n",
      "\n",
      "Explanation:\n",
      "The contexts discuss the treatment of necrotizing fasciitis (NF) and\n",
      "do not provide evidence that HBO is an indication for\n",
      "NF. Context 1 suggests that HBO is an adjuvant therapy\n",
      "for NF, but it also mentions that the beneficial effect\n",
      "of HBO for NF remains controversial. Context 2 and Context\n",
      "3 present a study comparing HBO-treated patients and non-HBO-treated patients\n",
      "for NF. The study did not find a significant difference\n",
      "in mortality rates between the two groups, with 36% mortality\n",
      "in the HBO group and 25% in the non-HBO group.\n",
      "Although HBO-treated patients required more surgical\n",
      "==============================\n",
      "\n",
      "\n",
      "Question 4: Can increases in the cigarette tax rate be linked to cigarette retail prices?\n",
      "\n",
      "==============================\n",
      "Question:\n",
      "Can increases in the cigarette tax rate be linked to cigarette retail prices?\n",
      "\n",
      "Predicted Answer: no\n",
      "Ground Truth: no\n",
      "\n",
      "Top Contexts:\n",
      "\n",
      "Context 1:\n",
      "To explain China's cigarette pricing mechanism and the role of\n",
      "the Chinese State Tobacco Monopoly Administration (STMA) on cigarette pricing\n",
      "and taxation.\n",
      "\n",
      "Context 2:\n",
      "Published government tobacco tax documentation and statistics published by the\n",
      "Chinese STMA are used to analyse the interrelations among industry\n",
      "profits, taxes and retail price of cigarettes in China.\n",
      "\n",
      "Context 3:\n",
      "The 2009 excise tax increase on cigarettes in China has\n",
      "not translated into higher retail prices because the Chinese STMA\n",
      "used its policy authority to ensure that retail cigarette prices\n",
      "did not change. The government tax increase is being collected\n",
      "at both the producer and wholesale levels. As a result,\n",
      "the 2009 excise tax increase in China has resulted in\n",
      "higher tax revenue for the government and lower profits for\n",
      "the tobacco industry, with no increase in the retail price\n",
      "of cigarettes for consumers.\n",
      "\n",
      "Explanation:\n",
      "The answer \"no\" is appropriate because the context provided focuses\n",
      "on the Chinese cigarette pricing mechanism and the impact of\n",
      "the 2009 excise tax increase on cigarette retail prices. According\n",
      "to the context, the Chinese State Tobacco Monopoly Administration (STMA)\n",
      "used its policy authority to ensure that retail cigarette prices\n",
      "did not increase despite the tax hike. The government collected\n",
      "the tax at the producer and wholesale levels, leading to\n",
      "higher tax revenue and lower industry profits without an increase\n",
      "in retail cigarette prices for consumers. Therefore, the context does\n",
      "not support a link between cigarette tax rate increases and\n",
      "retail prices\n",
      "==============================\n",
      "\n",
      "\n",
      "Question 5: Diagnostic and therapeutic ureteroscopy: is dilatation of ureteral meatus always necessary?\n",
      "\n",
      "==============================\n",
      "Question:\n",
      "Diagnostic and therapeutic ureteroscopy: is dilatation of ureteral meatus always necessary?\n",
      "\n",
      "Predicted Answer: no\n",
      "Ground Truth: no\n",
      "\n",
      "Top Contexts:\n",
      "\n",
      "Context 1:\n",
      "To assess the feasibility and safety of diagnostic or therapeutic\n",
      "semirigid ureteroscopy without ureteral meatus dilatation.\n",
      "\n",
      "Context 2:\n",
      "A comparative, retrospective study was conducted of patients undergoing ureteroscopy\n",
      "from January 2000 to May 2008. For data analysis purposes,\n",
      "the population was divided into two groups based on whether\n",
      "ureteroscopy had been performed with (Group 1) or without (Group\n",
      "2) ureteral meatus dilatation. Variables recorded included age, sex, type\n",
      "of procedure, surgical diagnosis, passive or active dilatation, number of\n",
      "stones, stone location, stone diameter, peroperative and postoperative complications, internal\n",
      "urinary diversion after the procedure, therapeutic success rate, operating time,\n",
      "and hospital stay duration. A 8-9.8 Fr Wolf semirigid ureteroscope\n",
      "was used. Descriptive statistics of the population and cohorts were\n",
      "performed, providing medians, quartiles, and limit values for non-normally distributed\n",
      "interval variables, and absolute and relative frequencies for categorical variables.\n",
      "Shapiro-Wilk's, Mann-Whitney's U, Chi-square, and Fisher's exact tests were used\n",
      "for statistical analysis. A value of p 2 alpha<or =\n",
      "0.005 was considered statistically significant. Arcus Quickstat Biomedical 1.0 software\n",
      "was used.\n",
      "\n",
      "Context 3:\n",
      "Among the 306 ureteroscopies studied, 286 performed in 256 patients\n",
      "were analyzed. Median age was 50 years (16-83), 59% of\n",
      "patients were male, and elective ureteroscopy was performed in 183\n",
      "patients (64%). Group 1: 191 ureteroscopies, Group 2: 95 ureteroscopies.\n",
      "Stone location: 149 in distal ureter, 60 in middle ureter,\n",
      "and 35 in proximal ureter. Sixty-nine percent of stones had\n",
      "sizes ranging from 5 and 10 mm. The overall success\n",
      "rate was 86.5%. There were 5 peroperative and 22 postoperative\n",
      "complications, with no statistically significant differences between the groups.\n",
      "\n",
      "Explanation:\n",
      "The context provided revolves around various studies and cases of\n",
      "ureteroscopy procedures, including diagnostic and therapeutic ureteroscopy. The main focus\n",
      "is on the feasibility, safety, and efficacy of these procedures\n",
      "without ureteral meatus dilatation. Context 1 explicitly states the aim\n",
      "of the study, which is to assess the feasibility and\n",
      "safety of diagnostic or therapeutic semirigid ureteroscopy without ureteral meatus\n",
      "dilatation. This indicates that the study aims to address the\n",
      "question without the need for dilatation. Context 3 presents data\n",
      "from 286 ureteroscopies, where no\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the labeled train set from PubMedQA\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\", split=\"train\")\n",
    "dataset = dataset.filter(lambda x: x[\"final_decision\"] in [\"yes\", \"no\", \"maybe\"])\n",
    "\n",
    "# Select 5 random indices\n",
    "random_indices = random.sample(range(len(dataset)), 5)\n",
    "\n",
    "# Run each selected question through the LangGraph pipeline\n",
    "for i, idx in enumerate(random_indices, 1):\n",
    "    question = dataset[idx][\"question\"]\n",
    "    print(f\"\\nQuestion {i}: {question}\")\n",
    "    _ = app.invoke({\"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVeWbjSPbaG6",
    "outputId": "cf3da35c-40b9-4fc1-9098-cf86a717e027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: Can vitamin D deficiency cause depression?\n",
      "\n",
      "==============================\n",
      "Question:\n",
      "Can vitamin D deficiency cause depression?\n",
      "\n",
      "Predicted Answer: yes\n",
      "Ground Truth: [unknown]\n",
      "\n",
      "Top Contexts:\n",
      "\n",
      "Context 1:\n",
      "Epidemiological data show significant associations of vitamin D deficiency and\n",
      "autoimmune diseases. Vitamin D may prevent autoimmunity by stimulating naturally\n",
      "occurring regulatory T cells.\n",
      "\n",
      "Context 2:\n",
      "To elucidate whether vitamin D supplementation increases Tregs frequency (%Tregs)\n",
      "within circulating CD4+ T cells.\n",
      "\n",
      "Context 3:\n",
      "We performed an uncontrolled vitamin D supplementation trial among 50\n",
      "apparently healthy subjects including supplementation of 140,000 IU at baseline\n",
      "and after 4 weeks (visit 1). The final follow-up visit\n",
      "was performed 8 weeks after the baseline examination (visit 2).\n",
      "Blood was drawn at each study visit to determine 25-hydroxyvitamin\n",
      "D levels and %Tregs. Tregs were characterized as CD4+CD25++ T\n",
      "cells with expression of the transcription factor forkhead box P3\n",
      "and low or absent expression of CD127.\n",
      "\n",
      "Explanation:\n",
      "The context provided discusses vitamin D deficiency, its impact on\n",
      "T-regulatory cells, and its prevalence in certain patient groups, such\n",
      "as those with chronic kidney disease (CKD) and osteochondritis dissecans\n",
      "(OCD). However, there is no direct evidence linking vitamin D\n",
      "deficiency to depression. The most relevant contexts (Context 3, 4)\n",
      "describe a study where vitamin D supplementation increased T-regulatory cells\n",
      "in participants. While T-regulatory cells play a role in immune\n",
      "system regulation and mental health, the study does not establish\n",
      "a causal relationship between vitamin D deficiency and depression. Context\n",
      "==============================\n",
      "\n",
      "\n",
      "Question 2: Is there a link between serotonin levels and depression?\n",
      "\n",
      "==============================\n",
      "Question:\n",
      "Is there a link between serotonin levels and depression?\n",
      "\n",
      "Predicted Answer: yes\n",
      "Ground Truth: [unknown]\n",
      "\n",
      "Top Contexts:\n",
      "\n",
      "Context 1:\n",
      "Evidence suggests substantial comorbidity between symptoms of somatization and depression\n",
      "in clinical as well as nonclinical populations. However, as most\n",
      "existing research has been retrospective or cross-sectional in design, very\n",
      "little is known about the specific nature of this relationship.\n",
      "In particular, it is unclear whether somatic complaints may heighten\n",
      "the risk for the subsequent development of depressive symptoms.\n",
      "\n",
      "Context 2:\n",
      "We report findings on the link between symptoms of somatization\n",
      "(assessed using the SCL-90-R) and depression 5 years later (assessed\n",
      "using the CES-D) in an initially healthy cohort of community\n",
      "adults, based on prospective data from the RENO Diet-Heart Study.\n",
      "\n",
      "Context 3:\n",
      "Gender-stratified multiple regression analyses revealed that baseline CES-D scores were\n",
      "the best predictors of subsequent depressive symptoms for men and\n",
      "women. Baseline scores on the SCL-90-R somatization subscale significantly predicted\n",
      "subsequent self-reported symptoms of depressed mood 5 years later, but\n",
      "only in women. However, somatic complaints were a somewhat less\n",
      "powerful predictor than income and age.\n",
      "\n",
      "Explanation:\n",
      "The link between serotonin levels and depression is suggested by\n",
      "the context, where it is mentioned that baseline CES-D scores,\n",
      "which are related to serotonin levels, significantly predicted subsequent self-reported\n",
      "symptoms of depressed mood 5 years later in women. This\n",
      "implies that serotonin levels, which are reflected in the CES-D\n",
      "scores, have an impact on depression. --- Note: The contexts\n",
      "provided do not explicitly mention serotonin levels. However, the explanation\n",
      "assumes a connection between CES-D scores and serotonin levels based\n",
      "on the general understanding in psychiatry. For a more accurate\n",
      "answer, explicit serotonin level data would be required. The\n",
      "==============================\n",
      "\n",
      "\n",
      "Question 3: Does cognitive behavioral therapy reduce symptoms of depression?\n",
      "\n",
      "==============================\n",
      "Question:\n",
      "Does cognitive behavioral therapy reduce symptoms of depression?\n",
      "\n",
      "Predicted Answer: yes\n",
      "Ground Truth: [unknown]\n",
      "\n",
      "Top Contexts:\n",
      "\n",
      "Context 1:\n",
      "There is increasing pressure on mental health providers to reduce\n",
      "the duration of treatments, while retaining level of quality and\n",
      "effectiveness. The risk is that the population is underserved and\n",
      "therefore needs new treatment episodes. The primary aim of this\n",
      "study was to investigate whether duration of treatment and return\n",
      "into mental health care were related.\n",
      "\n",
      "Context 2:\n",
      "This study examined Dutch patients with an initial treatment episode\n",
      "in 2009 or 2010 in specialized mental health settings for\n",
      "depressive disorder (N = 85,754). Follow-up data about treatment episodes\n",
      "were available up until 2013. The data set included demographic\n",
      "(age, gender), and clinical factors (comorbidity with other DSM-IV Axis;\n",
      "scores on the 'Global Assessment of Functioning'). Cox regression analyses\n",
      "were used to assess whether duration of treatment and relapse\n",
      "into mental health care were related.\n",
      "\n",
      "Context 3:\n",
      "The majority of patients did not return into mental health\n",
      "care (86 %). Patients with a shorter duration of treatment\n",
      "(5-250 min; 251-500 min and 751-1000 min) were slightly more\n",
      "likely to return (reference group:>1000 min) (HR 1.19 95 %\n",
      "CI 1.13-1.26; HR 1.11 95 % CI 1.06-1.17; HR 1.18\n",
      "95 % CI 1.11-1.25), adjusted for demographic and clinical variables.\n",
      "\n",
      "Explanation:\n",
      "--- Based on the provided contexts, cognitive behavioral therapy (CBT)\n",
      "is not explicitly mentioned. However, the contexts discuss depression and\n",
      "treatments for it, including the PHQ-9 for monitoring depression severity\n",
      "and measurement-based care approaches for depression treatment. Context 7 highlights\n",
      "the gap between evidence-based treatments and routine care for depression,\n",
      "emphasizing the importance of measurement-based care. CBT is an evidence-based\n",
      "treatment for depression. Although not directly mentioned, the context implies\n",
      "that implementing measurement-based care approaches, like using CBT, can improve\n",
      "treatment outcomes and potentially reduce symptoms of depression. Context 9\n",
      "describes the adoption of\n",
      "==============================\n",
      "\n",
      "\n",
      "Question 4: Can chronic inflammation contribute to the development of depression?\n",
      "\n",
      "==============================\n",
      "Question:\n",
      "Can chronic inflammation contribute to the development of depression?\n",
      "\n",
      "Predicted Answer: yes\n",
      "Ground Truth: [unknown]\n",
      "\n",
      "Top Contexts:\n",
      "\n",
      "Context 1:\n",
      "Evidence suggests substantial comorbidity between symptoms of somatization and depression\n",
      "in clinical as well as nonclinical populations. However, as most\n",
      "existing research has been retrospective or cross-sectional in design, very\n",
      "little is known about the specific nature of this relationship.\n",
      "In particular, it is unclear whether somatic complaints may heighten\n",
      "the risk for the subsequent development of depressive symptoms.\n",
      "\n",
      "Context 2:\n",
      "We report findings on the link between symptoms of somatization\n",
      "(assessed using the SCL-90-R) and depression 5 years later (assessed\n",
      "using the CES-D) in an initially healthy cohort of community\n",
      "adults, based on prospective data from the RENO Diet-Heart Study.\n",
      "\n",
      "Context 3:\n",
      "Gender-stratified multiple regression analyses revealed that baseline CES-D scores were\n",
      "the best predictors of subsequent depressive symptoms for men and\n",
      "women. Baseline scores on the SCL-90-R somatization subscale significantly predicted\n",
      "subsequent self-reported symptoms of depressed mood 5 years later, but\n",
      "only in women. However, somatic complaints were a somewhat less\n",
      "powerful predictor than income and age.\n",
      "\n",
      "Explanation:\n",
      "The classifier's answer is appropriate because Context 3 explicitly mentions\n",
      "that baseline scores on the SCL-90-0R somatization subscale significantly predicted\n",
      "subsequent self-reported symptoms of depressed mood 5 years later, specifically\n",
      "in women. Although the context discusses the prediction of depressive\n",
      "symptoms based on somatization in women, it does not directly\n",
      "mention \"chronic inflammation.\" However, somatization symptoms are often associated with\n",
      "underlying chronic inflammation. Since the question asks about the contribution\n",
      "of chronic inflammation to the development of depression, and the\n",
      "context shows a strong correlation between somatization symptoms and depression,\n",
      "it can be inferred that chron\n",
      "==============================\n",
      "\n",
      "\n",
      "Question 5: Is depression more prevalent in patients with chronic pain conditions?\n",
      "\n",
      "==============================\n",
      "Question:\n",
      "Is depression more prevalent in patients with chronic pain conditions?\n",
      "\n",
      "Predicted Answer: yes\n",
      "Ground Truth: [unknown]\n",
      "\n",
      "Top Contexts:\n",
      "\n",
      "Context 1:\n",
      "To study the prevalence of pain and risk factors for\n",
      "pain in psychiatric patients in a psychiatric hospital.\n",
      "\n",
      "Context 2:\n",
      "Using a questionnaire we investigated in a cross-sectional study the\n",
      "prevalence of pain, duration of pain, impairment and unfitness for\n",
      "work due to pain in 106 patients primarily diagnosed with\n",
      "a psychiatric disorder in the field of general adult psychiatry.\n",
      "Potential risk factors were explored.\n",
      "\n",
      "Context 3:\n",
      "The point prevalence of pain was about 50%, the 6-month\n",
      "prevalence 75.5% and the 12-month prevalence 76.5%. The patients' most\n",
      "frequent complaints were low back pain, headache and shoulder and\n",
      "neck pain. Patients with affective disorders most frequently had pain\n",
      "complaints, followed by those with neurotic, stress-related and somatoform disorders\n",
      "and those with psychotic disorders such as schizophrenia, schizotypic and\n",
      "delusional disorders. Almost 10% of all patients reported pain continuing\n",
      "at least 3 months in the past year. Impairment and\n",
      "unfitness for work were related to specific psychiatric diagnosis. Statistically\n",
      "significant risk factors for pain were depression (OR=6.05) and the\n",
      "number of past admissions to psychiatric hospitals (OR=3.609).\n",
      "\n",
      "Explanation:\n",
      "--- The classifier's predicted answer of \"yes\" is appropriate based\n",
      "on the context provided. The context (Context 3) specifically mentions\n",
      "that depression was found to be a statistically significant risk\n",
      "factor for pain in patients with various psychiatric disorders. This\n",
      "context directly correlates depression with pain prevalence in patients with\n",
      "psychiatric conditions. Although the context does not explicitly address chronic\n",
      "pain conditions, it does establish a link between depression and\n",
      "pain, which can be extrapolated to include chronic pain conditions\n",
      "due to the commonality of overlapping symptoms and conditions in\n",
      "biomedical research. It is important to note that while this\n",
      "context supports the answer, it does not definit\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define 5 custom biomedical questions related to depression\n",
    "depression_questions = [\n",
    "    \"Can vitamin D deficiency cause depression?\",\n",
    "    \"Is there a link between serotonin levels and depression?\",\n",
    "    \"Does cognitive behavioral therapy reduce symptoms of depression?\",\n",
    "    \"Can chronic inflammation contribute to the development of depression?\",\n",
    "    \"Is depression more prevalent in patients with chronic pain conditions?\"\n",
    "]\n",
    "\n",
    "# Send each question through the LangGraph RAG pipeline\n",
    "for i, q in enumerate(depression_questions, 1):\n",
    "    print(f\"\\nQuestion {i}: {q}\")\n",
    "    _ = app.invoke({\"question\": q})\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
