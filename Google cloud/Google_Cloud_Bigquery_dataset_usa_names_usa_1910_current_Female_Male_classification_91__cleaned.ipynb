{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUFXW5zLNKjA"
   },
   "source": [
    "### The project: Gender Classification from Baby Names Using BigQuery and XGBoost\n",
    "\n",
    "This project focuses on predicting the **gender (Male/Female)** of a given **baby name** based solely on its **textual structure**. It leverages the U.S. Social Security dataset `bigquery-public-data.usa_names.usa_1910_current`, which includes over a century of name-gender-frequency statistics.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective:\n",
    "Train a machine learning model that can **accurately classify names as either Male or Female** using character-based patterns and additional name features.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Preparation:\n",
    "1. Queried data from BigQuery: name, gender, and total occurrences.\n",
    "2. Filtered to include only names that are used **exclusively for one gender** (e.g., \"Emily\" is only Female).\n",
    "3. Removed rare names (used fewer than 100 times).\n",
    "4. Final dataset: **12,508 unique names**, balanced across genders.\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Engineering:\n",
    "- **Character-level n-grams**: extracted all 1–4 character sequences from names (e.g., `jo`, `ohn`, `lyn`).\n",
    "- **Manual features**:\n",
    "  - Name length\n",
    "  - First and last letters (one-hot encoded)\n",
    "  - Does the name end with `'a'`?\n",
    "  - Does the name contain `'y'`?\n",
    "\n",
    "All features were combined into a sparse feature matrix of shape `(12508, 15687)`.\n",
    "\n",
    "---\n",
    "\n",
    "### Modeling:\n",
    "- Used **XGBoost classifier** with hyperparameter tuning via **manual Grid Search** over 24 parameter combinations.\n",
    "- Evaluated each combination using 3-fold cross-validation.\n",
    "- Trained the best model on the full training set (80%) and evaluated on a 20% hold-out test set.\n",
    "\n",
    "---\n",
    "\n",
    "### Best Model Parameters:\n",
    "```python\n",
    "{\n",
    "  'n_estimators': 300,\n",
    "  'max_depth': 7,\n",
    "  'learning_rate': 0.2,\n",
    "  'subsample': 0.8,\n",
    "  'colsample_bytree': 0.8\n",
    "}\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "### Results:\n",
    "\n",
    "* **Cross-validated Accuracy (3-fold):** 88.79%\n",
    "* **Test Set Accuracy:** **90.61%**\n",
    "\n",
    "| Metric    | Female (F) | Male (M) |\n",
    "| --------- | ---------- | -------- |\n",
    "| Precision | 0.93       | 0.87     |\n",
    "| Recall    | 0.92       | 0.89     |\n",
    "| F1-score  | 0.92       | 0.88     |\n",
    "\n",
    "The model performs **very well** across both classes, with slightly better performance on female names. This is likely due to more consistent structural patterns in female naming (e.g., ending in `'a'`).\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "The classifier learned strong patterns from the **structure of names alone**, achieving over **90% accuracy** in predicting gender. This demonstrates the power of combining **text n-grams** and **symbolic name features** in a classical ML pipeline, without needing deep learning or external data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3NV6QsIHmzH"
   },
   "source": [
    "### The project: Predicting Gender from Baby Names Using BigQuery and XGBoost\n",
    "\n",
    "This project demonstrates how to build a gender classification model using U.S. baby name data from BigQuery. The goal is to predict whether a name is typically associated with male or female babies based on the structure and patterns within the name.\n",
    "\n",
    "#### Data Source:\n",
    "The dataset is queried directly from BigQuery's public dataset `usa_names.usa_1910_current`, which contains U.S. baby names, their gender, and yearly counts from 1910 onward.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-step Explanation:\n",
    "\n",
    "**Step 1 – Query Data from BigQuery**  \n",
    "The SQL query retrieves the total number of births for each unique (name, gender) combination, filtering out names with fewer than 100 total births to remove rare names.\n",
    "\n",
    "**Step 2 – Filter Gender-Specific Names**  \n",
    "Names that appear for both genders (e.g., Taylor, Jordan) are removed. Only names that appear exclusively for male or exclusively for female are kept, to increase classification clarity.\n",
    "\n",
    "**Step 3 – Prepare Labels**  \n",
    "The target variable (`gender`) is label-encoded: female (`F`) becomes 0, male (`M`) becomes 1.\n",
    "\n",
    "**Step 4 – N-gram Feature Extraction**  \n",
    "Character-level n-grams (1 to 3 characters) are extracted using `CountVectorizer`. This captures short letter patterns (e.g., \"ma\", \"ari\", \"son\") that may be informative for gender prediction.\n",
    "\n",
    "**Step 5 – Manual Feature Engineering**  \n",
    "Several handcrafted features are added:\n",
    "- `length`: length of the name\n",
    "- `first_letter`, `last_letter`: used to identify name structure\n",
    "- `ends_with_a`: many female names end in 'a'\n",
    "- `contains_y`: 'y' is common in certain male names\n",
    "\n",
    "The `first_letter` and `last_letter` are one-hot encoded to be used in the model.\n",
    "\n",
    "**Step 6 – Combine All Features**  \n",
    "All features (n-grams, one-hot letters, and numeric features) are merged into one sparse feature matrix using `hstack`.\n",
    "\n",
    "**Step 7 – Train/Test Split**  \n",
    "The dataset is split into training (80%) and testing (20%) sets.\n",
    "\n",
    "**Step 8 – Train XGBoost Model**  \n",
    "An XGBoost classifier is trained using 300 estimators, a maximum tree depth of 6, and a learning rate of 0.1. The model is trained on the combined feature matrix.\n",
    "\n",
    "**Step 9 – Evaluation**  \n",
    "Model performance is evaluated using accuracy and a classification report. This includes precision, recall, and F1-score for each gender.\n",
    "\n",
    "---\n",
    "\n",
    "### Result:\n",
    "The model typically achieves accuracy above **90%**, depending on parameters and feature richness. It effectively captures linguistic patterns in names to predict gender with high precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePye3Np-HWaR"
   },
   "source": [
    "# The dataset: `bigquery-public-data.usa_names.usa_1910_current`\n",
    "\n",
    "This dataset contains historical records of baby names registered in the United States from the year 1910 onwards. It is publicly hosted on Google BigQuery as part of Google's public datasets.\n",
    "\n",
    "## Source\n",
    "\n",
    "The data originates from the U.S. Social Security Administration (SSA) and includes aggregated birth name information by year, gender, and state.\n",
    "\n",
    "## Columns Description\n",
    "\n",
    "| Column   | Type    | Description |\n",
    "|----------|---------|-------------|\n",
    "| `name`   | STRING  | The first name of the baby. |\n",
    "| `gender` | STRING  | The gender associated with the name: either `\"M\"` (Male) or `\"F\"` (Female). |\n",
    "| `state`  | STRING  | The U.S. state abbreviation (e.g., \"CA\", \"NY\"). In some entries this field may be null. |\n",
    "| `year`   | INTEGER | The year in which the name was registered. |\n",
    "| `number` | INTEGER | The number of babies given this name in that year and state. |\n",
    "\n",
    "## Target / Label Column\n",
    "\n",
    "There is no explicit label column in this dataset, as it is not originally designed for supervised machine learning tasks. However, depending on the goal of the analysis, one can define a target. For example:\n",
    "- In a **gender prediction task**, the column `gender` can serve as the label.\n",
    "- For **popularity prediction**, the column `number` or its change over time could be used as a target variable.\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "This dataset is suitable for a variety of data science and analytics tasks, including:\n",
    "- Trend analysis of baby name popularity over time.\n",
    "- Gender distribution of names.\n",
    "- Identification of unisex (gender-neutral) names.\n",
    "- Forecasting future popularity of specific names.\n",
    "- Sociocultural studies (e.g., names influenced by events, celebrities, or politics).\n",
    "\n",
    "## Dataset Characteristics\n",
    "\n",
    "- Total rows: ~1.8 million (depending on filters).\n",
    "- Time range: 1910 to present.\n",
    "- Granularity: Yearly.\n",
    "- Aggregated: Not individual-level data, but grouped by name/year/state/gender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fOf1DwjZFRth"
   },
   "outputs": [],
   "source": [
    "# Step 1: Install required packages\n",
    "!pip install -q google-cloud-bigquery xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jv_rCf-oEmS4"
   },
   "outputs": [],
   "source": [
    "# Step 2: Authenticate and import libraries\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KmJJrXeeFbwX"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Step 3: Connect to BigQuery and query gender + name data\n",
    "project_id = \"custom-helix-474006-k6\"  # ← replace with your GCP project ID\n",
    "client = bigquery.Client(project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vyrXtV1Gt4A",
    "outputId": "5baebd59-a9a3-4f8c-8d6e-cdefb0ab8df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.21 %\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.92      0.91      0.91      1556\n",
      "           M       0.85      0.87      0.86       946\n",
      "\n",
      "    accuracy                           0.89      2502\n",
      "   macro avg       0.88      0.89      0.89      2502\n",
      "weighted avg       0.89      0.89      0.89      2502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT name, gender, SUM(number) AS total\n",
    "FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
    "GROUP BY name, gender\n",
    "HAVING total > 100\n",
    "\"\"\"\n",
    "df = client.query(query).to_dataframe()\n",
    "\n",
    "# Step 4: Filter gender-specific names\n",
    "gender_counts = df.groupby(\"name\")[\"gender\"].nunique().reset_index()\n",
    "gender_counts = gender_counts[gender_counts[\"gender\"] == 1]\n",
    "df = df[df[\"name\"].isin(gender_counts[\"name\"])].reset_index(drop=True)\n",
    "\n",
    "# Step 5: Prepare labels and normalize names\n",
    "df[\"name\"] = df[\"name\"].str.lower()\n",
    "y = LabelEncoder().fit_transform(df[\"gender\"])  # F=0, M=1\n",
    "\n",
    "# Step 6: n-gram vectorization (char-level 1 to 3)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "X_ngram = vectorizer.fit_transform(df[\"name\"])\n",
    "\n",
    "# Step 7: Manual features\n",
    "df[\"length\"] = df[\"name\"].str.len()\n",
    "df[\"first_letter\"] = df[\"name\"].str[0]\n",
    "df[\"last_letter\"] = df[\"name\"].str[-1]\n",
    "df[\"ends_with_a\"] = (df[\"last_letter\"] == 'a').astype(int)\n",
    "df[\"contains_y\"] = df[\"name\"].str.contains(\"y\").astype(int)\n",
    "\n",
    "# One-hot encode first and last letters\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
    "letter_ohe = ohe.fit_transform(df[[\"first_letter\", \"last_letter\"]])\n",
    "\n",
    "# Combine manual numeric features\n",
    "X_manual = df[[\"length\", \"ends_with_a\", \"contains_y\"]].values\n",
    "\n",
    "# Combine all features: n-gram + one-hot + manual\n",
    "X_combined = hstack([X_ngram, letter_ohe, X_manual])\n",
    "\n",
    "# Step 8: Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 9: Train XGBoost\n",
    "model = XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.1, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 10: Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred) * 100, 2), \"%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"F\", \"M\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CS-1-coqJ_Zu",
    "outputId": "10868f95-da95-4e9e-d522-984217ecdcbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Querying data from BigQuery...\n",
      "Retrieved 14692 rows.\n",
      "Step 2: Filtering names used for only one gender...\n",
      "Remaining after filtering: 12508 names.\n",
      "Step 3: Encoding gender labels (F=0, M=1)...\n",
      "Step 4: Extracting character-level n-grams (1 to 4)...\n",
      "Number of n-gram features: 15632\n",
      "Step 5: Adding manual features...\n",
      "Step 6: One-hot encoding first and last letters...\n",
      "One-hot shape: (12508, 52)\n",
      "Manual numeric feature shape: (12508, 3)\n",
      "Step 7: Combining all features...\n",
      "Final feature matrix shape: (12508, 15687)\n",
      "Step 8: Splitting data into train and test sets...\n",
      "Training samples: 10006 | Test samples: 2502\n",
      "Step 9: Running manual GridSearch on XGBoost with live output...\n",
      "Total combinations: 24\n",
      "\n",
      "Running combination 1/24: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8773 ± 0.0043\n",
      "\n",
      "Running combination 2/24: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8771 ± 0.0054\n",
      "\n",
      "Running combination 3/24: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8792 ± 0.0051\n",
      "\n",
      "Running combination 4/24: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 1.0, 'colsample_bytree': 1.0, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8760 ± 0.0046\n",
      "\n",
      "Running combination 5/24: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8849 ± 0.0046\n",
      "\n",
      "Running combination 6/24: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2, 'subsample': 0.8, 'colsample_bytree': 1.0, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8830 ± 0.0062\n",
      "\n",
      "Running combination 7/24: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8831 ± 0.0068\n",
      "\n",
      "Running combination 8/24: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2, 'subsample': 1.0, 'colsample_bytree': 1.0, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8847 ± 0.0061\n",
      "\n",
      "Running combination 9/24: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8811 ± 0.0056\n",
      "\n",
      "Running combination 10/24: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8815 ± 0.0037\n",
      "\n",
      "Running combination 11/24: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8813 ± 0.0038\n",
      "\n",
      "Running combination 12/24: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 1.0, 'colsample_bytree': 1.0, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8811 ± 0.0047\n",
      "\n",
      "Running combination 13/24: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8837 ± 0.0070\n",
      "\n",
      "Running combination 14/24: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 0.8, 'colsample_bytree': 1.0, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8853 ± 0.0048\n",
      "\n",
      "Running combination 15/24: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8843 ± 0.0051\n",
      "\n",
      "Running combination 16/24: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 1.0, 'colsample_bytree': 1.0, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8836 ± 0.0080\n",
      "\n",
      "Running combination 17/24: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8806 ± 0.0068\n",
      "\n",
      "Running combination 18/24: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8829 ± 0.0031\n",
      "\n",
      "Running combination 19/24: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8848 ± 0.0073\n",
      "\n",
      "Running combination 20/24: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 1.0, 'colsample_bytree': 1.0, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8817 ± 0.0068\n",
      "\n",
      "Running combination 21/24: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8879 ± 0.0036\n",
      "\n",
      "Running combination 22/24: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2, 'subsample': 0.8, 'colsample_bytree': 1.0, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8850 ± 0.0053\n",
      "\n",
      "Running combination 23/24: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2, 'subsample': 1.0, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8867 ± 0.0079\n",
      "\n",
      "Running combination 24/24: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2, 'subsample': 1.0, 'colsample_bytree': 1.0, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "  -> Accuracy: 0.8873 ± 0.0055\n",
      "\n",
      "Step 10: Evaluating best model...\n",
      "Best parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.8, 'use_label_encoder': False, 'eval_metric': 'logloss', 'n_jobs': -1}\n",
      "Cross-validated accuracy: 88.79%\n",
      "\n",
      "Test Accuracy: 90.61%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.93      0.92      0.92      1556\n",
      "           M       0.87      0.89      0.88       946\n",
      "\n",
      "    accuracy                           0.91      2502\n",
      "   macro avg       0.90      0.90      0.90      2502\n",
      "weighted avg       0.91      0.91      0.91      2502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import hstack\n",
    "from itertools import product\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Step 1: Querying data from BigQuery...\")\n",
    "query = \"\"\"\n",
    "SELECT name, gender, SUM(number) AS total\n",
    "FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
    "GROUP BY name, gender\n",
    "HAVING total > 100\n",
    "\"\"\"\n",
    "df = client.query(query).to_dataframe()\n",
    "print(f\"Retrieved {len(df)} rows.\")\n",
    "\n",
    "print(\"Step 2: Filtering names used for only one gender...\")\n",
    "gender_counts = df.groupby(\"name\")[\"gender\"].nunique().reset_index()\n",
    "gender_counts = gender_counts[gender_counts[\"gender\"] == 1]\n",
    "df = df[df[\"name\"].isin(gender_counts[\"name\"])].reset_index(drop=True)\n",
    "print(f\"Remaining after filtering: {len(df)} names.\")\n",
    "\n",
    "print(\"Step 3: Encoding gender labels (F=0, M=1)...\")\n",
    "df[\"name\"] = df[\"name\"].str.lower()\n",
    "y = LabelEncoder().fit_transform(df[\"gender\"])\n",
    "\n",
    "print(\"Step 4: Extracting character-level n-grams (1 to 4)...\")\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 4))\n",
    "X_ngram = vectorizer.fit_transform(df[\"name\"])\n",
    "print(f\"Number of n-gram features: {X_ngram.shape[1]}\")\n",
    "\n",
    "print(\"Step 5: Adding manual features...\")\n",
    "df[\"length\"] = df[\"name\"].str.len()\n",
    "df[\"first_letter\"] = df[\"name\"].str[0]\n",
    "df[\"last_letter\"] = df[\"name\"].str[-1]\n",
    "df[\"ends_with_a\"] = (df[\"last_letter\"] == 'a').astype(int)\n",
    "df[\"contains_y\"] = df[\"name\"].str.contains(\"y\").astype(int)\n",
    "\n",
    "print(\"Step 6: One-hot encoding first and last letters...\")\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
    "letter_ohe = ohe.fit_transform(df[[\"first_letter\", \"last_letter\"]])\n",
    "print(f\"One-hot shape: {letter_ohe.shape}\")\n",
    "\n",
    "X_manual = df[[\"length\", \"ends_with_a\", \"contains_y\"]].values\n",
    "print(f\"Manual numeric feature shape: {X_manual.shape}\")\n",
    "\n",
    "print(\"Step 7: Combining all features...\")\n",
    "X_combined = hstack([X_ngram, letter_ohe, X_manual])\n",
    "print(f\"Final feature matrix shape: {X_combined.shape}\")\n",
    "\n",
    "print(\"Step 8: Splitting data into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training samples: {X_train.shape[0]} | Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "print(\"Step 9: Running manual GridSearch on XGBoost with live output...\")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [300],\n",
    "    \"max_depth\": [5, 6, 7],\n",
    "    \"learning_rate\": [0.1, 0.2],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "all_combinations = list(product(\n",
    "    param_grid[\"n_estimators\"],\n",
    "    param_grid[\"max_depth\"],\n",
    "    param_grid[\"learning_rate\"],\n",
    "    param_grid[\"subsample\"],\n",
    "    param_grid[\"colsample_bytree\"]\n",
    "))\n",
    "\n",
    "results = []\n",
    "print(f\"Total combinations: {len(all_combinations)}\\n\")\n",
    "\n",
    "for i, (n_estimators, max_depth, lr, subsample, colsample) in enumerate(all_combinations, 1):\n",
    "    params = {\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"learning_rate\": lr,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample,\n",
    "        \"use_label_encoder\": False,\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "\n",
    "    print(f\"Running combination {i}/{len(all_combinations)}: {params}\")\n",
    "    model = XGBClassifier(**params)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "    print(f\"  -> Accuracy: {mean_score:.4f} ± {std_score:.4f}\\n\")\n",
    "\n",
    "    results.append((params, mean_score, std_score))\n",
    "\n",
    "# Step 10: Select best model and evaluate\n",
    "print(\"Step 10: Evaluating best model...\")\n",
    "\n",
    "# Find best result\n",
    "best_result = max(results, key=lambda x: x[1])  # highest mean_score\n",
    "best_params, best_score, best_std = best_result\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Cross-validated accuracy: {round(best_score * 100, 2)}%\")\n",
    "\n",
    "# Train best model on full training set\n",
    "best_model = XGBClassifier(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"\\nTest Accuracy: {round(accuracy_score(y_test, y_pred) * 100, 2)}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"F\", \"M\"]))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
